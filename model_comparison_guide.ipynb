{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step-by-Step Guide: Comparing RLiG with Baselines\n",
    "\n",
    "This notebook provides a systematic approach to comparing RLiG with baseline models. We'll go through the process step by step, making sure everything is working properly at each stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Environment Setup\n",
    "\n",
    "First, let's make sure we have all the necessary dependencies installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install pgmpy sklearn pandas numpy tqdm tensorflow matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Install causalnex for NOTEARS algorithm\n",
    "!pip install causalnex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Install ucimlrepo for accessing UCI datasets\n",
    "!pip install ucimlrepo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and suppress warnings\n",
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "logging.getLogger('pgmpy').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Install and Check RLiG\n",
    "\n",
    "We need to ensure the RLiG package is properly installed. Since it's a local package, we'll need to install it from the local directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install RLiG locally\n",
    "!cd ../ganblr-0.1.1 && pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify RLiG installation\n",
    "try:\n",
    "    from ganblr.models import RLiG\n",
    "    print(\"RLiG successfully imported!\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing RLiG: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Dataset Preparation\n",
    "\n",
    "Let's prepare our datasets. We'll use a couple of small datasets first to verify everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to get UCI datasets\n",
    "def get_dataset(name, id=None, path=None):\n",
    "    \"\"\"\n",
    "    Get a dataset by name, either from UCI repository or local path\n",
    "    \"\"\"\n",
    "    if id is not None:\n",
    "        try:\n",
    "            from ucimlrepo import fetch_ucirepo\n",
    "            dataset = fetch_ucirepo(id=id)\n",
    "            X = dataset.data.features\n",
    "            y = dataset.data.targets\n",
    "            print(f\"Loaded {name} dataset from UCI ML Repository\")\n",
    "            print(f\"Features shape: {X.shape}, Target shape: {y.shape}\")\n",
    "            return X, y\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading UCI dataset: {e}\")\n",
    "    \n",
    "    if path is not None:\n",
    "        try:\n",
    "            df = pd.read_csv(path)\n",
    "            X = df.iloc[:, :-1]\n",
    "            y = df.iloc[:, -1:]\n",
    "            print(f\"Loaded {name} dataset from {path}\")\n",
    "            print(f\"Features shape: {X.shape}, Target shape: {y.shape}\")\n",
    "            return X, y\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading dataset from file: {e}\")\n",
    "    \n",
    "    # If we get here, we couldn't load the dataset\n",
    "    print(f\"Could not load {name} dataset\")\n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test loading a dataset\n",
    "X, y = get_dataset(\"Rice\", id=545)\n",
    "if X is not None:\n",
    "    print(\"\\nFeature columns:\", X.columns.tolist())\n",
    "    print(\"\\nFirst few rows of X:\")\n",
    "    display(X.head())\n",
    "    print(\"\\nFirst few rows of y:\")\n",
    "    display(y.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Data Preprocessing\n",
    "\n",
    "We need to preprocess our data to ensure it's in the right format for all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import preprocessing packages\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, KBinsDiscretizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode_cols(X, cols):\n",
    "    \"\"\"Label encode categorical columns\"\"\"\n",
    "    X_encoded = X.copy()\n",
    "    encoders = {}\n",
    "    for col in cols:\n",
    "        le = LabelEncoder()\n",
    "        X_encoded[col] = le.fit_transform(X_encoded[col])\n",
    "        encoders[col] = le\n",
    "    return X_encoded, encoders\n",
    "\n",
    "def preprocess_data(X, y):\n",
    "    \"\"\"Preprocess data: discretize continuous variables and encode categoricals\"\"\"\n",
    "    # Identify column types\n",
    "    continuous_cols = X.select_dtypes(include=['number']).columns\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "    \n",
    "    print(f\"Continuous columns: {len(continuous_cols)}\")\n",
    "    print(f\"Categorical columns: {len(categorical_cols)}\")\n",
    "    \n",
    "    # Create transformation pipeline\n",
    "    transformers = []\n",
    "    if len(continuous_cols) > 0:\n",
    "        continuous_transformer = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('discretizer', KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform'))\n",
    "        ])\n",
    "        transformers.append(('num', continuous_transformer, continuous_cols))\n",
    "    \n",
    "    # Handle categorical columns\n",
    "    if len(categorical_cols) > 0:\n",
    "        X, encoders = label_encode_cols(X, categorical_cols)\n",
    "    \n",
    "    # Apply transformations\n",
    "    preprocessor = ColumnTransformer(transformers=transformers, remainder='passthrough')\n",
    "    X_transformed = preprocessor.fit_transform(X)\n",
    "    X_transformed_df = pd.DataFrame(X_transformed, columns=continuous_cols.tolist() + categorical_cols.tolist())\n",
    "    \n",
    "    # Handle target variable\n",
    "    if y.dtypes[0] == 'object':\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_transformed = pd.DataFrame(label_encoder.fit_transform(y.values.ravel()), columns=y.columns)\n",
    "    else:\n",
    "        y_transformed = y\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_transformed_df, y_transformed, test_size=0.2, random_state=42, stratify=y)\n",
    "    print(f\"Training set: {X_train.shape[0]} samples\")\n",
    "    print(f\"Test set: {X_test.shape[0]} samples\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test preprocessing on our dataset\n",
    "if X is not None and y is not None:\n",
    "    X_train, X_test, y_train, y_test = preprocess_data(X, y)\n",
    "    \n",
    "    print(\"\\nPreprocessed training data (first few rows):\")\n",
    "    display(X_train.head())\n",
    "    \n",
    "    # Create combined dataset for pgmpy\n",
    "    train_data = pd.concat([X_train, y_train], axis=1)\n",
    "    print(\"\\nCombined training data shape:\", train_data.shape)\n",
    "    display(train_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Testing the Baseline Models\n",
    "\n",
    "Let's start by testing the baseline models one by one to make sure each works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import model packages\n",
    "from pgmpy.estimators import HillClimbSearch, BicScore, TreeSearch, MaximumLikelihoodEstimator\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.inference import VariableElimination\n",
    "from pgmpy.metrics import structure_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for model evaluation\n",
    "def train_bn(model, data):\n",
    "    \"\"\"Train a Bayesian Network model\"\"\"\n",
    "    bn = BayesianNetwork()\n",
    "    bn.add_nodes_from(model.nodes())\n",
    "    bn.add_edges_from(model.edges())\n",
    "    \n",
    "    # Fit model using Maximum Likelihood Estimation\n",
    "    try:\n",
    "        bn.fit(data, estimator=MaximumLikelihoodEstimator)\n",
    "        return bn\n",
    "    except Exception as e:\n",
    "        print(f\"Error fitting Bayesian Network: {e}\")\n",
    "        return None\n",
    "\n",
    "def evaluate_bn_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluate Bayesian Network classification performance\"\"\"\n",
    "    if model is None:\n",
    "        return None\n",
    "    \n",
    "    infer = VariableElimination(model)\n",
    "    target_var = y_test.columns[0]  # Assumes only one target variable\n",
    "    model_nodes = set(model.nodes())\n",
    "    y_pred = []\n",
    "    \n",
    "    # Make predictions for each test instance\n",
    "    for index, row in X_test.iterrows():\n",
    "        evidence = {k: v for k, v in row.to_dict().items() if k in model_nodes}\n",
    "        try:\n",
    "            q = infer.map_query(variables=[target_var], evidence=evidence, show_progress=False)\n",
    "            y_pred.append(q[target_var])\n",
    "        except Exception as e:\n",
    "            y_pred.append(None)\n",
    "    \n",
    "    # Filter out None predictions\n",
    "    y_test_classes = y_test[target_var].unique()\n",
    "    y_pred = [pred if pred in y_test_classes else None for pred in y_pred]\n",
    "    \n",
    "    valid_indices = [i for i, pred in enumerate(y_pred) if pred is not None]\n",
    "    if not valid_indices:\n",
    "        return 0.0  # No valid predictions\n",
    "    \n",
    "    y_pred = [y_pred[i] for i in valid_indices]\n",
    "    y_test_filtered = y_test.iloc[valid_indices].values.ravel()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    return accuracy_score(y_test_filtered, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Hill Climbing Search\n",
    "if 'train_data' in locals():\n",
    "    print(\"Testing Hill Climbing Search...\")\n",
    "    try:\n",
    "        hc = HillClimbSearch(train_data)\n",
    "        best_model_hc = hc.estimate(scoring_method=BicScore(train_data))\n",
    "        print(f\"Hill Climbing Search model has {len(best_model_hc.nodes())} nodes and {len(best_model_hc.edges())} edges\")\n",
    "        print(f\"Edges: {best_model_hc.edges()}\")\n",
    "        \n",
    "        # Train the model\n",
    "        bn_hc = train_bn(best_model_hc, train_data)\n",
    "        \n",
    "        # Evaluate\n",
    "        if bn_hc:\n",
    "            hc_acc = evaluate_bn_model(bn_hc, X_test, y_test)\n",
    "            hc_bic = structure_score(bn_hc, train_data, scoring_method=\"bic\")\n",
    "            print(f\"Hill Climbing - Accuracy: {hc_acc:.4f}, BIC: {hc_bic}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with Hill Climbing Search: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Tree Search\n",
    "if 'train_data' in locals():\n",
    "    print(\"Testing Tree Search...\")\n",
    "    try:\n",
    "        ts = TreeSearch(train_data)\n",
    "        best_model_ts = ts.estimate()\n",
    "        print(f\"Tree Search model has {len(best_model_ts.nodes())} nodes and {len(best_model_ts.edges())} edges\")\n",
    "        print(f\"Edges: {best_model_ts.edges()}\")\n",
    "        \n",
    "        # Train the model\n",
    "        bn_ts = train_bn(best_model_ts, train_data)\n",
    "        \n",
    "        # Evaluate\n",
    "        if bn_ts:\n",
    "            ts_acc = evaluate_bn_model(bn_ts, X_test, y_test)\n",
    "            ts_bic = structure_score(bn_ts, train_data, scoring_method=\"bic\")\n",
    "            print(f\"Tree Search - Accuracy: {ts_acc:.4f}, BIC: {ts_bic}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with Tree Search: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Naive Bayes\n",
    "if 'X_train' in locals() and 'y_train' in locals():\n",
    "    print(\"Testing Naive Bayes...\")\n",
    "    try:\n",
    "        nb = GaussianNB()\n",
    "        nb.fit(X_train, y_train.values.ravel())\n",
    "        \n",
    "        # Evaluate\n",
    "        y_pred = nb.predict(X_test)\n",
    "        nb_acc = accuracy_score(y_test.values.ravel(), y_pred)\n",
    "        print(f\"Naive Bayes - Accuracy: {nb_acc:.4f}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with Naive Bayes: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test NOTEARS (if causalnex is available)\n",
    "try:\n",
    "    from causalnex.structure import StructureModel\n",
    "    from causalnex.structure.notears import from_pandas\n",
    "    \n",
    "    if 'train_data' in locals():\n",
    "        print(\"Testing NOTEARS...\")\n",
    "        try:\n",
    "            sm = from_pandas(train_data, w_threshold=0.8)\n",
    "            print(f\"NOTEARS model has {len(sm.nodes)} nodes and {len(sm.edges)} edges\")\n",
    "            \n",
    "            # Train the model\n",
    "            bn_nt = train_bn(sm, train_data)\n",
    "            \n",
    "            # Evaluate\n",
    "            if bn_nt:\n",
    "                nt_acc = evaluate_bn_model(bn_nt, X_test, y_test)\n",
    "                nt_bic = structure_score(bn_nt, train_data, scoring_method=\"bic\")\n",
    "                print(f\"NOTEARS - Accuracy: {nt_acc:.4f}, BIC: {nt_bic}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error with NOTEARS: {e}\")\n",
    "except ImportError:\n",
    "    print(\"CausalNex is not installed. NOTEARS will be skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Testing RLiG\n",
    "\n",
    "Now let's test RLiG to ensure it works properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test RLiG on original data format (not the preprocessed version)\n",
    "if X is not None and y is not None:\n",
    "    print(\"Testing RLiG...\")\n",
    "    try:\n",
    "        from ganblr.models import RLiG\n",
    "        \n",
    "        # Initialize model\n",
    "        rlig_model = RLiG()\n",
    "        \n",
    "        # Fit model - use small values for testing\n",
    "        print(\"Fitting RLiG model (this may take a while)...\")\n",
    "        rlig_model.fit(X, y, episodes=2, gan=1, k=0, epochs=5, n=1)\n",
    "        \n",
    "        # Evaluate using RLiG's built-in evaluation\n",
    "        print(\"Evaluating RLiG model...\")\n",
    "        lr_result = rlig_model.evaluate(X, y, model='lr')\n",
    "        mlp_result = rlig_model.evaluate(X, y, model='mlp')\n",
    "        rf_result = rlig_model.evaluate(X, y, model='rf')\n",
    "        \n",
    "        print(f\"RLiG - LR Accuracy: {lr_result:.4f}\")\n",
    "        print(f\"RLiG - MLP Accuracy: {mlp_result:.4f}\")\n",
    "        print(f\"RLiG - RF Accuracy: {rf_result:.4f}\")\n",
    "        \n",
    "        # Visualize the learned Bayesian network\n",
    "        if hasattr(rlig_model, 'bayesian_network'):\n",
    "            model_graphviz = rlig_model.bayesian_network.to_graphviz()\n",
    "            model_graphviz.draw(\"rlig_network.png\", prog=\"dot\")\n",
    "            print(\"RLiG network visualization saved to rlig_network.png\")\n",
    "            \n",
    "            # Print network properties\n",
    "            print(f\"RLiG model has {len(rlig_model.bayesian_network.nodes())} nodes and {len(rlig_model.bayesian_network.edges())} edges\")\n",
    "            \n",
    "            # BIC score\n",
    "            if hasattr(rlig_model, 'best_score'):\n",
    "                print(f\"RLiG BIC score: {rlig_model.best_score}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error with RLiG: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Running the Full Comparison\n",
    "\n",
    "Now that we've verified each model works individually, let's run the full comparison framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the full comparison framework\n",
    "import sys\n",
    "sys.path.append('../')  # Adjust this path as needed\n",
    "from compare_models import compare_models, format_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define datasets to evaluate\n",
    "# Start with small datasets for testing\n",
    "datasets = {\n",
    "    'Rice': 545,       # UCI ID for Rice dataset\n",
    "    'TicTacToe': 101   # UCI ID for Tic-tac-toe dataset\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the comparison\n",
    "results = compare_models(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format and display results\n",
    "formatted_results = format_results(results)\n",
    "\n",
    "print(\"\\n\\n=== ACCURACY RESULTS ===\")\n",
    "print(formatted_results['accuracy'])\n",
    "print(\"\\n\\n=== TIME RESULTS (seconds) ===\")\n",
    "print(formatted_results['time'])\n",
    "print(\"\\n\\n=== BIC SCORE RESULTS ===\")\n",
    "print(formatted_results['bic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the accuracy results\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = formatted_results['accuracy'].plot(kind='bar')\n",
    "plt.title('Model Accuracy Comparison')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Dataset')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend(title='Model')\n",
    "\n",
    "# Add value labels on the bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.2f', padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('accuracy_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the time results\n",
    "plt.figure(figsize=(12, 6))\n",
    "ax = formatted_results['time'].plot(kind='bar')\n",
    "plt.title('Model Training Time Comparison (seconds)')\n",
    "plt.ylabel('Time (s)')\n",
    "plt.xlabel('Dataset')\n",
    "plt.xticks(rotation=0)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.legend(title='Model')\n",
    "\n",
    "# Add value labels on the bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.1f', padding=3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('time_comparison.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Troubleshooting\n",
    "\n",
    "If you encounter issues, these debugging cells may help identify the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check RLiG details\n",
    "import inspect\n",
    "try:\n",
    "    from ganblr.models import RLiG\n",
    "    print(\"RLiG methods:\")\n",
    "    for method_name, method in inspect.getmembers(RLiG, predicate=inspect.isfunction):\n",
    "        print(f\"  - {method_name}\")\n",
    "except ImportError as e:\n",
    "    print(f\"Error importing RLiG: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for CPD normalization issues (mentioned in your question)\n",
    "if 'rlig_model' in locals() and hasattr(rlig_model, 'bayesian_network'):\n",
    "    print(\"Checking CPDs in RLiG model...\")\n",
    "    for cpd in rlig_model.bayesian_network.get_cpds():\n",
    "        node = cpd.variable\n",
    "        values = cpd.values\n",
    "        sum_values = np.sum(values, axis=0)\n",
    "        norm_condition = np.allclose(sum_values, 1.0)\n",
    "        print(f\"Node: {node}, Values shape: {values.shape}, Sum to 1: {norm_condition}\")\n",
    "        if not norm_condition:\n",
    "            print(f\"  - Issue detected: Sum values = {sum_values}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 9: Next Steps\n",
    "\n",
    "Once you've confirmed everything works with these small datasets, you can expand to the full set of 13 datasets mentioned in the paper.\n",
    "\n",
    "1. Add more datasets to the datasets dictionary\n",
    "2. Run the full comparison\n",
    "3. Analyze the results across all datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For full paper replication, use all 13 datasets\n",
    "# This is just a template - adjust paths as needed\n",
    "all_datasets = {\n",
    "    'Adult': 2,\n",
    "    'Rice': 545,\n",
    "    'TicTacToe': 101,\n",
    "    # Add other datasets here\n",
    "    # 'Dataset_name': id_or_path\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Run the full comparison (may take a long time)\n",
    "# Uncomment to run\n",
    "# full_results = compare_models(all_datasets)\n",
    "# full_formatted_results = format_results(full_results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}