# Model Evaluation Results Summary

## Overview

- Total datasets evaluated: 15
- Total models compared: 1
- Overall average performance: 0.4466

## Model Rankings

| Rank | Model | Average Score |
|------|-------|---------------|
| 1 | TABDIFF | 0.4466 |

## Top Performing Model-Classifier Combinations

| Rank | Model-Classifier | Average Score |
|------|-----------------|---------------|
| 1 | TABDIFF-MLP | 0.4854 |
| 2 | TABDIFF-LR | 0.4795 |
| 3 | TABDIFF-XGB | 0.4564 |
| 4 | TABDIFF-RF | 0.3738 |

## Dataset Performance

Best performing model for each dataset:

| Dataset | Best Model | Score |
|---------|------------|-------|
| Rice | TABDIFF | 0.6507 |
| Car | TABDIFF | 0.5595 |
| TicTacToe | TABDIFF | 0.5868 |
| Chess | TABDIFF | 0.5273 |
| Magic | TABDIFF | 0.5390 |
| letter_recog | TABDIFF | 0.0177 |
| connect4 | TABDIFF | 0.1329 |
| maternal_health | TABDIFF | 0.3399 |
| nursery | TABDIFF | 0.2820 |
| room_occupancy | TABDIFF | 0.3373 |
| adult | TABDIFF | 0.6331 |
| default | TABDIFF | 0.6664 |
| nsl-kdd | TABDIFF | 0.5390 |
| credit | TABDIFF | 0.5598 |
| covtype | TABDIFF | 0.3280 |
