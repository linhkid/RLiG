{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Bayesian Networks for Classification\n",
    "by Jaime Blackwell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaime/repos/sit723/venv_3.8/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R packages including CAM have been already installed.\n",
      "importing R packages CAM and mboost\n"
     ]
    }
   ],
   "source": [
    "# from sklearn import datasets\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, KBinsDiscretizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from pgmpy.estimators import HillClimbSearch, BicScore, TreeSearch, MaximumLikelihoodEstimator\n",
    "from pgmpy.models import BayesianNetwork\n",
    "from pgmpy.inference import VariableElimination\n",
    "from causalnex.structure import StructureModel\n",
    "from causalnex.structure.notears import from_pandas\n",
    "from ucimlrepo import fetch_ucirepo \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Import CDRL from trustworthyAI\n",
    "sys.path.append('/Users/jaime/repos/sit723/trustworthyAI/research/Causal Discovery with RL/src')\n",
    "\n",
    "\n",
    "input_file = '/Users/jaime/repos/sit723/causal-datasets/Real_Dataset/real_dataset_processed.csv'\n",
    "if not os.path.isfile(input_file):\n",
    "    raise ValueError(\"Input file does not exist: {}\".format(input_file))\n",
    "\n",
    "data = pd.read_csv('/Users/jaime/repos/sit723/causal-datasets/Real_Dataset/real_dataset_processed.csv')\n",
    "np.save(\"/Users/jaime/repos/sit723/causal-datasets/Real_Dataset/DAG.npy\", data)\n",
    "\n",
    "sys.argv = [\n",
    "    'main.py',\n",
    "    '--input_file', input_file,\n",
    "    '--output_dir', '/Users/jaime/repos/sit723/output',\n",
    "    '--seed', '42'\n",
    "]\n",
    "\n",
    "import main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = {\n",
    "            # 'Adult': 2,     # Adult https://archive.ics.uci.edu/dataset/2/adult     14 features, 48,842 instances\n",
    "            # 'Breast Cancer': 17,    # Breast Cancer Wisconsin (Diagnostic) https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic    30 features, 569 instances\n",
    "            # 'Magic': 159,   # Magic: https://archive.ics.uci.edu/dataset/159/magic+gamma+telescope      10 features, 19,020 instances\n",
    "            # 'Raisin':  850,     # Raisin https://archive.ics.uci.edu/dataset/850/raisin     7 features, 900 instances\n",
    "            'Rice': 545,       # Rice https://archive.ics.uci.edu/dataset/545/rice+cammeo+and+osmancik   7 features,  3810 instances\n",
    "            'TicTacToe': 101   # Tic tac toe https://archive.ics.uci.edu/dataset/101/tic+tac+toe+endgame   9 features, 958 instances\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discretize the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode_cols(X, cols):\n",
    "    X_encoded = X.copy()\n",
    "    encoders = {}\n",
    "    for col in cols:\n",
    "        le = LabelEncoder()\n",
    "        X_encoded[col] = le.fit_transform(X_encoded[col])\n",
    "        encoders[col] = le\n",
    "    return X_encoded, encoders\n",
    "\n",
    "\n",
    "def preprocess_data(X, y):   # Discretize and encode dataset as required\n",
    "    continuous_cols = X.select_dtypes(include=['number']).columns\n",
    "    categorical_cols = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "    transformers = []\n",
    "\n",
    "    if len(continuous_cols) > 0:\n",
    "        continuous_transformer = Pipeline(steps=[\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('discretizer', KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform'))\n",
    "        ])\n",
    "        transformers.append(('num', continuous_transformer, continuous_cols))\n",
    "\n",
    "    if len(categorical_cols) > 0:\n",
    "        X, encoders = label_encode_cols(X, categorical_cols)\n",
    "\n",
    "    preprocessor = ColumnTransformer(transformers=transformers, remainder='passthrough')\n",
    "    X_transformed = preprocessor.fit_transform(X)\n",
    "    X_transformed_df = pd.DataFrame(X_transformed, columns=continuous_cols.tolist() + categorical_cols.tolist())\n",
    "\n",
    "    if y.dtypes[0] == 'object':\n",
    "        label_encoder = LabelEncoder()\n",
    "        y_transformed = pd.DataFrame(label_encoder.fit_transform(y.values.ravel()), columns=y.columns)\n",
    "    else:\n",
    "        y_transformed = y\n",
    "\n",
    "    return train_test_split(X_transformed_df, y_transformed, test_size=0.2, random_state=42, stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_test, y_test):  # Evaluate each DAG based on classification task\n",
    "    infer = VariableElimination(model)\n",
    "    target_var = y_test.columns[0]  # Assumes only one target variable\n",
    "    model_nodes = set(model.nodes())\n",
    "    y_pred = []\n",
    "\n",
    "    for index, row in X_test.iterrows():\n",
    "        evidence = {k: v for k, v in row.to_dict().items() if k in model_nodes}\n",
    "        try:\n",
    "            q = infer.map_query(variables=[target_var], evidence=evidence, show_progress=False)  # Maximum a posteriori\n",
    "            y_pred.append(q[target_var])\n",
    "        except IndexError as e: y_pred.append(None)\n",
    "        except ValueError as e: y_pred.append(None)\n",
    "        except Exception as e: y_pred.append(None)\n",
    "\n",
    "    y_test_classes = y_test[target_var].unique()\n",
    "    y_pred = [pred if pred in y_test_classes else None for pred in y_pred]\n",
    "\n",
    "    valid_indices = [i for i, pred in enumerate(y_pred) if pred is not None]\n",
    "    y_pred = [y_pred[i] for i in valid_indices]\n",
    "    y_test = y_test.iloc[valid_indices].values.ravel()  # Ensure y_test is a 1D array\n",
    "    y_pred = pd.Series(y_pred).values  # Ensure y_pred is a 1D array\n",
    "\n",
    "    pd.DataFrame(y_test, columns=[target_var]).to_csv('y_test.csv', index=False)    # Save y_test and y_pred to CSV\n",
    "    y_pred_df = pd.DataFrame(y_pred, columns=['Class'])\n",
    "    y_pred_df.to_csv('y_pred.csv', index=False)\n",
    "    print(f\"Length of y_pred: {len(y_pred)}\")\n",
    "\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "def evaluate_naive_bayes(model, X_test, y_test):     # Evaluate the NB model\n",
    "    y_pred = model.predict(X_test)\n",
    "    return accuracy_score(y_test, y_pred)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bn(model, data):        # Train Bayesian Networks\n",
    "    # \\TRAIN\n",
    "    bn = BayesianNetwork()\n",
    "    bn.add_nodes_from(model.nodes())\n",
    "    bn.add_edges_from(model.edges())\n",
    "    # FIT\n",
    "    bn.fit(data, estimator=MaximumLikelihoodEstimator)\n",
    "    return bn\n",
    "\n",
    "def get_results_table(datasets):\n",
    "    \n",
    "    results_dict = {}\n",
    "    for name, id in datasets.items():\n",
    "        data = fetch_ucirepo(id=id)\n",
    "\n",
    "        X = data.data.features \n",
    "        y = data.data.targets\n",
    "\n",
    "        X_train, X_test, y_train, y_test = preprocess_data(X, y) # Preprocess data\n",
    "        train_data = pd.concat([X_train, y_train], axis=1)\n",
    "        train_data.to_csv(f\"{name}_train_data.csv\")     # export training data post processing \n",
    "\n",
    "        hc = HillClimbSearch(train_data)        # Hill Climbing\n",
    "        best_model_hc = hc.estimate(scoring_method=BicScore(train_data))\n",
    "\n",
    "        ts = TreeSearch(train_data)     # Tree Search\n",
    "        best_model_ts = ts.estimate()\n",
    "\n",
    "        nb = GaussianNB()       # Naive Bayes \n",
    "        \n",
    "        sm = from_pandas(train_data, w_threshold=0.8)   # NOTEARS structure model\n",
    "\n",
    "        # TRAIN\n",
    "        bn_hc = train_bn(best_model_hc, train_data)\n",
    "        bn_ts = train_bn(best_model_ts, train_data)\n",
    "        bn_nt = train_bn(sm, train_data)    # Train BN for NOTEARS bayesian network\n",
    "\n",
    "        # FIT\n",
    "        # bn_hc.fit(train_data)\n",
    "        # bn_ts.fit(train_data)\n",
    "        nb.fit(X_train, y_train)\n",
    "        # bn_nt.fit(train_data, estimator=MaximumLikelihoodEstimator)\n",
    "\n",
    "        # EVALUATE\n",
    "        accuracy_hc = evaluate_model(bn_hc, X_test, y_test)\n",
    "        accuracy_ts = evaluate_model(bn_ts, X_test, y_test)\n",
    "        accuracy_nb = evaluate_naive_bayes(nb, X_test, y_test)\n",
    "        accuracy_nt = evaluate_model(bn_nt, X_test, y_test)\n",
    "\n",
    "        # Collate results dict\n",
    "        dataset_dict = {'Hill Climbing': accuracy_hc\n",
    "                      ,'Tree Search':  accuracy_ts\n",
    "                      ,'Naive Bayes': accuracy_nb\n",
    "                      ,'NOTEARS': accuracy_nt\n",
    "                      }\n",
    "        print(f\"{name}: {dataset_dict}\")\n",
    "        results_dict[name] = dataset_dict\n",
    "\n",
    "    results_df = pd.DataFrame.from_dict(results_dict, orient='index')  # Put results into dataframe\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jaime/repos/sit723/venv_3.8/lib/python3.8/site-packages/sklearn/preprocessing/_discretization.py:248: FutureWarning: In version 1.5 onwards, subsample=200_000 will be used by default. Set subsample explicitly to silence this warning in the mean time. Set subsample=None to disable subsampling explicitly.\n",
      "  warnings.warn(\n",
      "  0%|          | 9/1000000 [00:00<4:44:15, 58.63it/s]\n",
      "Building tree: 100%|██████████| 28/28.0 [00:00<00:00, 3313.22it/s]\n",
      "/Users/jaime/repos/sit723/venv_3.8/lib/python3.8/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of y_pred: 762\n",
      "Length of y_pred: 762\n",
      "Length of y_pred: 762\n",
      "Rice: {'Hill Climbing': 0.8543307086614174, 'Tree Search': 0.8451443569553806, 'Naive Bayes': 0.8727034120734908, 'NOTEARS': 0.5196850393700787}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 10/1000000 [00:00<5:47:33, 47.95it/s]\n",
      "Building tree: 100%|██████████| 45/45.0 [00:00<00:00, 2353.56it/s]\n",
      "/Users/jaime/repos/sit723/venv_3.8/lib/python3.8/site-packages/sklearn/utils/validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of y_pred: 192\n",
      "Length of y_pred: 192\n",
      "Length of y_pred: 192\n",
      "TicTacToe: {'Hill Climbing': 0.703125, 'Tree Search': 0.703125, 'Naive Bayes': 0.7083333333333334, 'NOTEARS': 0.6666666666666666}\n"
     ]
    }
   ],
   "source": [
    "accuracy_df = get_results_table(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Hill Climbing  Tree Search  Naive Bayes   NOTEARS\n",
      "Rice            0.854331     0.845144     0.872703  0.519685\n",
      "TicTacToe       0.703125     0.703125     0.708333  0.666667\n",
      "\\begin{tabular}{lrrrr}\n",
      " & Hill Climbing & Tree Search & Naive Bayes & NOTEARS \\\\\n",
      "Rice & 0.854331 & 0.845144 & 0.872703 & 0.519685 \\\\\n",
      "TicTacToe & 0.703125 & 0.703125 & 0.708333 & 0.666667 \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_df)  # Display the DataFrame\n",
    "print(accuracy_df.style.to_latex())   # get latex"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_sit723",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
